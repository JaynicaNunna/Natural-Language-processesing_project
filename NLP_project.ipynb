{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8019295,
          "sourceType": "datasetVersion",
          "datasetId": 4725197
        }
      ],
      "dockerImageVersionId": 30698,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook1f9f9fa0b4",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'amazon-product-reviews:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4725197%2F8019295%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240423%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240423T213545Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1d3206684062f33cc8383a82dd1037707498acac06eacd292333c4d8158790c4d96fa98e3830dc885dd361b268ec578b4b6f7281e8a0433cb068df21cd50e190f4a1a42457c56abefc005d712188521608cf3470bf8d863dbaa52ddcfe3f41fc7d77cecd1ce80e90ecf31640242c58f7d0f21965ea009ade01cbfaad54ba0b9646b431a595b6a5b3fea57da5478786155f6b16f6c09e9dac1b830170ee530b163872d5eaff102564a8900493bf816198ffe6638af3cc437887f6679b162fe1c1e127bc355df09fc0efbc6b287b717a29fb4a385f30bee4e088a52296202a5f43ccac994b268022b7d3a140985d2569c6d6e0c11baae26721ea8678eeeba5693b'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "moVlb2mS4vKB"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "gkxP1yTu4vKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:17:50.362286Z",
          "iopub.execute_input": "2024-04-23T21:17:50.362647Z",
          "iopub.status.idle": "2024-04-23T21:18:00.379654Z",
          "shell.execute_reply.started": "2024-04-23T21:17:50.362621Z",
          "shell.execute_reply": "2024-04-23T21:18:00.378636Z"
        },
        "trusted": true,
        "id": "s1mx8I1Q4vKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/kaggle/input/amazon-product-reviews/ratings_Electronics (1).csv', header=None)\n",
        "df.columns = ['user_id', 'prod_id', 'rating', 'timestamp']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:20:46.477923Z",
          "iopub.execute_input": "2024-04-23T21:20:46.478396Z",
          "iopub.status.idle": "2024-04-23T21:20:54.05368Z",
          "shell.execute_reply.started": "2024-04-23T21:20:46.47836Z",
          "shell.execute_reply": "2024-04-23T21:20:54.052446Z"
        },
        "trusted": true,
        "id": "ploqqLth4vKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check the shape of the sampled dataframe\n",
        "print(\" DataFrame Shape:\", df.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:20:55.735219Z",
          "iopub.execute_input": "2024-04-23T21:20:55.735628Z",
          "iopub.status.idle": "2024-04-23T21:20:55.741051Z",
          "shell.execute_reply.started": "2024-04-23T21:20:55.735594Z",
          "shell.execute_reply": "2024-04-23T21:20:55.739967Z"
        },
        "trusted": true,
        "id": "4DXDFP0K4vKL",
        "outputId": "abd1e81b-f380-4c76-f0bf-55eed925bcbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": " DataFrame Shape: (7824482, 4)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the last 7320000 rows from the DataFrame\n",
        "df = df[:-7824382]\n",
        "\n",
        "# Check the shape of the reduced dataframe\n",
        "print(\"Reduced DataFrame Shape:\", df.shape)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:21:24.259117Z",
          "iopub.execute_input": "2024-04-23T21:21:24.25952Z",
          "iopub.status.idle": "2024-04-23T21:21:24.267273Z",
          "shell.execute_reply.started": "2024-04-23T21:21:24.259492Z",
          "shell.execute_reply": "2024-04-23T21:21:24.265623Z"
        },
        "trusted": true,
        "id": "04hC1cEo4vKN",
        "outputId": "1009746e-a55e-4c50-f8b7-605876720982"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Reduced DataFrame Shape: (100, 4)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check the shape of the sampled dataframe\n",
        "print(\" DataFrame Shape:\", df.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:21:40.446248Z",
          "iopub.execute_input": "2024-04-23T21:21:40.446667Z",
          "iopub.status.idle": "2024-04-23T21:21:40.453451Z",
          "shell.execute_reply.started": "2024-04-23T21:21:40.446638Z",
          "shell.execute_reply": "2024-04-23T21:21:40.452194Z"
        },
        "trusted": true,
        "id": "jyIikMMi4vKO",
        "outputId": "4e6954ac-8004-45a2-d96e-a43e6092abc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": " DataFrame Shape: (100, 4)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocess the dataset (convert 'rating' to text data and 'prod_id' as labels)\n",
        "df['text'] = df['rating'].astype(str)\n",
        "df['label'] = df['prod_id']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:21:43.50443Z",
          "iopub.execute_input": "2024-04-23T21:21:43.505218Z",
          "iopub.status.idle": "2024-04-23T21:21:43.516281Z",
          "shell.execute_reply.started": "2024-04-23T21:21:43.505181Z",
          "shell.execute_reply": "2024-04-23T21:21:43.514999Z"
        },
        "trusted": true,
        "id": "Lc_Tkkpd4vKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:21:46.956608Z",
          "iopub.execute_input": "2024-04-23T21:21:46.957454Z",
          "iopub.status.idle": "2024-04-23T21:21:46.970698Z",
          "shell.execute_reply.started": "2024-04-23T21:21:46.957402Z",
          "shell.execute_reply": "2024-04-23T21:21:46.969757Z"
        },
        "trusted": true,
        "id": "VyoX4ORW4vKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the class labels\n",
        "class_labels = train_df['label'].unique().tolist()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:21:51.288965Z",
          "iopub.execute_input": "2024-04-23T21:21:51.289347Z",
          "iopub.status.idle": "2024-04-23T21:21:51.298007Z",
          "shell.execute_reply.started": "2024-04-23T21:21:51.289316Z",
          "shell.execute_reply": "2024-04-23T21:21:51.296943Z"
        },
        "trusted": true,
        "id": "Ul8kNWAT4vKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:21:54.081562Z",
          "iopub.execute_input": "2024-04-23T21:21:54.081964Z",
          "iopub.status.idle": "2024-04-23T21:21:54.450533Z",
          "shell.execute_reply.started": "2024-04-23T21:21:54.081933Z",
          "shell.execute_reply": "2024-04-23T21:21:54.449111Z"
        },
        "trusted": true,
        "id": "X8aO1NG44vKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and encode the text data\n",
        "def tokenize_text(df, tokenizer, max_length):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in df['text']:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                            str(text),\n",
        "                            add_special_tokens=True,\n",
        "                            max_length=max_length,\n",
        "                            padding='max_length',\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt',\n",
        "                       )\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "max_length = 128  # Maximum sequence length\n",
        "batch_size = 32   # Batch size for training\n",
        "\n",
        "# Tokenize and encode training and testing data\n",
        "train_inputs, train_masks = tokenize_text(train_df, tokenizer, max_length)\n",
        "test_inputs, test_masks = tokenize_text(test_df, tokenizer, max_length)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:21:59.778809Z",
          "iopub.execute_input": "2024-04-23T21:21:59.779222Z",
          "iopub.status.idle": "2024-04-23T21:21:59.827928Z",
          "shell.execute_reply.started": "2024-04-23T21:21:59.77919Z",
          "shell.execute_reply": "2024-04-23T21:21:59.825974Z"
        },
        "trusted": true,
        "id": "NO-oOV6O4vKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # Import tqdm for progress bar\n",
        "\n",
        "def tokenize_text(df, tokenizer, max_length):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # Use tqdm to create a progress bar\n",
        "    progress_bar = tqdm(total=len(df), desc=\"Tokenizing Text\")\n",
        "\n",
        "    for text in df['text']:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                            str(text),\n",
        "                            add_special_tokens=True,\n",
        "                            max_length=max_length,\n",
        "                            padding='max_length',\n",
        "                            return_attention_mask=True,\n",
        "                            return_tensors='pt',\n",
        "                       )\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        # Update the progress bar\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Close the progress bar after completion\n",
        "    progress_bar.close()\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:22:04.723133Z",
          "iopub.execute_input": "2024-04-23T21:22:04.723514Z",
          "iopub.status.idle": "2024-04-23T21:22:04.732681Z",
          "shell.execute_reply.started": "2024-04-23T21:22:04.723484Z",
          "shell.execute_reply": "2024-04-23T21:22:04.731324Z"
        },
        "trusted": true,
        "id": "AwbAmtDC4vKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the tokenize_text function with your DataFrame, tokenizer, and max_length parameters\n",
        "train_inputs, train_masks = tokenize_text(train_df, tokenizer, max_length)\n",
        "test_inputs, test_masks = tokenize_text(test_df, tokenizer, max_length)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:22:08.845953Z",
          "iopub.execute_input": "2024-04-23T21:22:08.846459Z",
          "iopub.status.idle": "2024-04-23T21:22:08.895992Z",
          "shell.execute_reply.started": "2024-04-23T21:22:08.846423Z",
          "shell.execute_reply": "2024-04-23T21:22:08.894416Z"
        },
        "trusted": true,
        "id": "xmpHv2iZ4vKS",
        "outputId": "a04589be-9879-4dfe-921d-15d4d91caced"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Tokenizing Text: 100%|██████████| 80/80 [00:00<00:00, 3104.36it/s]\nTokenizing Text: 100%|██████████| 20/20 [00:00<00:00, 2861.83it/s]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to PyTorch tensors, handling unknown labels\n",
        "train_labels = torch.tensor(train_df['label'].apply(lambda x: class_labels.index(x) if x in class_labels else -1).tolist())\n",
        "test_labels = torch.tensor(test_df['label'].apply(lambda x: class_labels.index(x) if x in class_labels else -1).tolist())\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:22:17.589761Z",
          "iopub.execute_input": "2024-04-23T21:22:17.590542Z",
          "iopub.status.idle": "2024-04-23T21:22:17.598536Z",
          "shell.execute_reply.started": "2024-04-23T21:22:17.590494Z",
          "shell.execute_reply": "2024-04-23T21:22:17.597186Z"
        },
        "trusted": true,
        "id": "WIQGJS8J4vKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader for training and testing sets\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:22:19.797849Z",
          "iopub.execute_input": "2024-04-23T21:22:19.798428Z",
          "iopub.status.idle": "2024-04-23T21:22:19.811593Z",
          "shell.execute_reply.started": "2024-04-23T21:22:19.798385Z",
          "shell.execute_reply": "2024-04-23T21:22:19.809655Z"
        },
        "trusted": true,
        "id": "fnLMq4EC4vKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=len(class_labels),  # Number of output classes\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False,\n",
        ")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:22:22.197187Z",
          "iopub.execute_input": "2024-04-23T21:22:22.197635Z",
          "iopub.status.idle": "2024-04-23T21:22:25.689538Z",
          "shell.execute_reply.started": "2024-04-23T21:22:22.197603Z",
          "shell.execute_reply": "2024-04-23T21:22:25.688183Z"
        },
        "trusted": true,
        "id": "kvglisLk4vKT",
        "outputId": "c5335735-2e7d-43ac-e1cf-db95a552e712"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "epochs = 4  # Number of training epochs\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:22:29.220884Z",
          "iopub.execute_input": "2024-04-23T21:22:29.221349Z",
          "iopub.status.idle": "2024-04-23T21:22:29.235194Z",
          "shell.execute_reply.started": "2024-04-23T21:22:29.221315Z",
          "shell.execute_reply": "2024-04-23T21:22:29.233576Z"
        },
        "trusted": true,
        "id": "-tAA7OKb4vKT",
        "outputId": "267861c6-e275-4a43-b5c0-90b9a7f061b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Train the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "total_epochs = 3  # Set the total number of epochs\n",
        "\n",
        "for epoch in range(total_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Integrate with tqdm for progress bar\n",
        "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{total_epochs}', leave=False)\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(b_input_ids,\n",
        "                        token_type_ids=None,\n",
        "                        attention_mask=b_input_mask,\n",
        "                        labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({'Training loss': total_train_loss / len(progress_bar)})\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    print(f'Epoch {epoch + 1}:')\n",
        "    print(f'  Training loss: {avg_train_loss:.2f}')\n",
        "\n",
        "print(\"Training completed!\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:22:31.830183Z",
          "iopub.execute_input": "2024-04-23T21:22:31.830634Z",
          "iopub.status.idle": "2024-04-23T21:24:47.011671Z",
          "shell.execute_reply.started": "2024-04-23T21:22:31.830603Z",
          "shell.execute_reply": "2024-04-23T21:24:47.010542Z"
        },
        "trusted": true,
        "id": "ftJhEbNI4vKT",
        "outputId": "f8191ed9-d460-4573-e804-61b9364133de"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "                                                                            \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1:\n  Training loss: 2.95\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                             \r",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2:\n  Training loss: 2.79\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "                                                                            ",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3:\n  Training loss: 2.71\nTraining completed!\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\r",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to('cpu').numpy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(b_input_ids,\n",
        "                        token_type_ids=None,\n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    predictions.extend(logits)\n",
        "    true_labels.extend(b_labels)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:24:53.208831Z",
          "iopub.execute_input": "2024-04-23T21:24:53.209515Z",
          "iopub.status.idle": "2024-04-23T21:24:57.12837Z",
          "shell.execute_reply.started": "2024-04-23T21:24:53.20948Z",
          "shell.execute_reply": "2024-04-23T21:24:57.127133Z"
        },
        "trusted": true,
        "id": "5OwPaDEQ4vKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:25:00.290611Z",
          "iopub.execute_input": "2024-04-23T21:25:00.291475Z",
          "iopub.status.idle": "2024-04-23T21:25:00.299698Z",
          "shell.execute_reply.started": "2024-04-23T21:25:00.291434Z",
          "shell.execute_reply": "2024-04-23T21:25:00.298467Z"
        },
        "trusted": true,
        "id": "6r181lSk4vKU",
        "outputId": "495b4c93-5d19-4236-b469-1a2d08e0b788"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy: 0.3000\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_t5 = T5Tokenizer.from_pretrained('t5-base')\n",
        "model_t5 = T5ForConditionalGeneration.from_pretrained('t5-base').to(device)\n",
        "\n",
        "# Define problematic class labels\n",
        "problematic_labels = ['0594296420', '0439886341']\n",
        "\n",
        "# Generate summaries for each predicted class\n",
        "for i, class_label in enumerate(class_labels):\n",
        "    if class_label in problematic_labels:\n",
        "        continue  # Skip problematic class labels\n",
        "\n",
        "    # Filter test data for the current class label\n",
        "    test_data_class = test_df[test_df['label'] == class_label]['text'].tolist()\n",
        "\n",
        "    # Check if test data for the current class label is empty\n",
        "    if not test_data_class:\n",
        "        continue  # Skip if empty\n",
        "\n",
        "    # Tokenize and encode the text data for T5 input\n",
        "    input_ids_t5 = tokenizer_t5.batch_encode_plus(test_data_class, return_tensors='pt', max_length=512, truncation=True, padding='longest').input_ids.to(device)\n",
        "\n",
        "    # Generate summaries\n",
        "    with torch.no_grad():\n",
        "        output = model_t5.generate(input_ids=input_ids_t5, max_length=150, num_beams=2, early_stopping=True)\n",
        "\n",
        "    # Decode the generated summaries\n",
        "    summaries = [tokenizer_t5.decode(summary, skip_special_tokens=True) for summary in output]\n",
        "\n",
        "    # Print the summaries\n",
        "    print(f\"Class Label: {class_label}\")\n",
        "    for summary in summaries:\n",
        "        print(summary)\n",
        "    print(\"-----------------------------------------------------\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T21:32:23.722185Z",
          "iopub.execute_input": "2024-04-23T21:32:23.722616Z",
          "iopub.status.idle": "2024-04-23T21:33:24.555942Z",
          "shell.execute_reply.started": "2024-04-23T21:32:23.722583Z",
          "shell.execute_reply": "2024-04-23T21:33:24.554734Z"
        },
        "trusted": true,
        "id": "Hlw6_Dol4vKV",
        "outputId": "56b6dcbf-e9f4-45db-bc54-538758a7adb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Class Label: 0594033896\n4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0 4.0\n-----------------------------------------------------\nClass Label: 0594451647\n1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n-----------------------------------------------------\nClass Label: 0528881469\n5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0\n2.0, 2.0 (2.02.0, 2.0 2.0 ( 2.0) 2.0 ( 2.0 2.0 ( 2.0) 2.0 ( 2.0) 2.0 ( 2.0 ( 2.0) 2.0 ( 2.0 2.0 2.0) 2.0 ( 2.0 ( 2.0 2.0 2.0) 2.0 ( 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0\n5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0\n2.0, 2.0 (2.02.0, 2.0 2.0 ( 2.0) 2.0 ( 2.0 2.0 ( 2.0) 2.0 ( 2.0) 2.0 ( 2.0 ( 2.0) 2.0 ( 2.0 2.0 2.0) 2.0 ( 2.0 ( 2.0 2.0 2.0) 2.0 ( 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0 2.0\n1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n-----------------------------------------------------\nClass Label: 0594012015\n1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n-----------------------------------------------------\nClass Label: 0594033926\n5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0\n-----------------------------------------------------\nClass Label: 059400232X\n5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0\n-----------------------------------------------------\nClass Label: 0511189877\n5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0\n-----------------------------------------------------\nClass Label: 0594033934\n5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0\n-----------------------------------------------------\nClass Label: 0594450209\n5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0\n-----------------------------------------------------\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}