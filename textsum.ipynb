{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54b208",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.002598,
     "end_time": "2024-04-23T22:19:29.872712",
     "exception": false,
     "start_time": "2024-04-23T22:19:29.870114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ab194",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T22:02:52.809051Z",
     "iopub.status.busy": "2024-04-23T22:02:52.808675Z",
     "iopub.status.idle": "2024-04-23T22:07:19.120019Z",
     "shell.execute_reply": "2024-04-23T22:07:19.117810Z",
     "shell.execute_reply.started": "2024-04-23T22:02:52.809022Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-04-23T22:19:29.875056",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/kaggle/input/amazon-product-reviews/ratings_Electronics (1).csv', header=None)\n",
    "df.columns = ['user_id', 'prod_id', 'rating', 'timestamp']\n",
    "\n",
    "# Drop unnecessary rows\n",
    "df = df[:-7824382]\n",
    "\n",
    "# Preprocess the dataset\n",
    "df['text'] = df['rating'].astype(str)\n",
    "df['label'] = df['prod_id']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the class labels\n",
    "class_labels = train_df['label'].unique().tolist()\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Tokenize and encode the text data for classification\n",
    "def tokenize_text(df, tokenizer, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in df['text']:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            str(text),\n",
    "                            add_special_tokens=True,\n",
    "                            max_length=max_length,\n",
    "                            padding='max_length',\n",
    "                            return_attention_mask=True,\n",
    "                            return_tensors='pt',\n",
    "                       )\n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "max_length = 128  # Maximum sequence length\n",
    "batch_size = 32   # Batch size for training\n",
    "\n",
    "# Tokenize and encode training and testing data for classification\n",
    "train_inputs, train_masks = tokenize_text(train_df, tokenizer, max_length)\n",
    "test_inputs, test_masks = tokenize_text(test_df, tokenizer, max_length)\n",
    "\n",
    "# Convert labels to PyTorch tensors, handling unknown labels\n",
    "train_labels = torch.tensor(train_df['label'].apply(lambda x: class_labels.index(x) if x in class_labels else -1).tolist())\n",
    "test_labels = torch.tensor(test_df['label'].apply(lambda x: class_labels.index(x) if x in class_labels else -1).tolist())\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "# Load the pre-trained BERT model for sequence classification\n",
    "model_classification = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', \n",
    "    num_labels=len(class_labels),  # Number of output classes\n",
    "    output_attentions=False, \n",
    "    output_hidden_states=False, \n",
    ")\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model_classification.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 4  # Number of training epochs\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Train the classification model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_classification.to(device)\n",
    "\n",
    "total_epochs = 3  # Set the total number of epochs\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    model_classification.train()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    # Integrate with tqdm for progress bar\n",
    "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{total_epochs}', leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model_classification.zero_grad()        \n",
    "\n",
    "        outputs = model_classification(b_input_ids, \n",
    "                                       token_type_ids=None, \n",
    "                                       attention_mask=b_input_mask, \n",
    "                                       labels=b_labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_classification.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'Training loss': total_train_loss / len(progress_bar)})\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    print(f'Epoch {epoch + 1}:')\n",
    "    print(f'  Training loss: {avg_train_loss:.2f}')\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Evaluate the classification model on the test set\n",
    "model_classification.eval()\n",
    "\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to('cpu').numpy()\n",
    "\n",
    "    with torch.no_grad():        \n",
    "        outputs = model_classification(b_input_ids, \n",
    "                                       token_type_ids=None, \n",
    "                                       attention_mask=b_input_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    predictions.extend(logits)\n",
    "    true_labels.extend(b_labels)\n",
    "    \n",
    "# Calculate accuracy\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Load the T5 tokenizer and model for summarization\n",
    "tokenizer_summarization = T5Tokenizer.from_pretrained('t5-base')\n",
    "model_summarization = T5ForConditionalGeneration.from_pretrained('t5-base').to(device)\n",
    "\n",
    "# Generate summaries for each class label\n",
    "for class_label in class_labels:\n",
    "    # Filter test data for the current class label\n",
    "    test_data_class = test_df[test_df['label'] == class_label]['text'].tolist()\n",
    "    \n",
    "    # Tokenize and encode the text data for T5 input\n",
    "    input_ids_t5 = tokenizer_summarization.batch_encode_plus(test_data_class, \n",
    "                                                             return_tensors='pt', \n",
    "                                                             max_length=512, \n",
    "                                                             truncation=True, \n",
    "                                                             padding='longest').input_ids.to(device)\n",
    "    \n",
    "    # Generate summaries\n",
    "    with torch.no_grad():\n",
    "        output = model_summarization.generate(input_ids=input_ids_t5, \n",
    "                                               max_length=150, \n",
    "                                               num_beams=2, \n",
    "                                               early_stopping=True)\n",
    "        \n",
    "    # Decode the generated summaries\n",
    "    summaries = [tokenizer_summarization.decode(summary, skip_special_tokens=True) for summary in output]\n",
    "\n",
    "    # Print the class label and its corresponding summaries\n",
    "    print(f\"Class Label: {class_label}\")\n",
    "    for summary in summaries:\n",
    "        print(summary)\n",
    "    print(\"-----------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99994863",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T22:16:29.820325Z",
     "iopub.status.busy": "2024-04-23T22:16:29.819833Z",
     "iopub.status.idle": "2024-04-23T22:17:53.406866Z",
     "shell.execute_reply": "2024-04-23T22:17:53.405606Z",
     "shell.execute_reply.started": "2024-04-23T22:16:29.820288Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the T5 tokenizer and model for summarization\n",
    "tokenizer_summarization = T5Tokenizer.from_pretrained('t5-base')\n",
    "model_summarization = T5ForConditionalGeneration.from_pretrained('t5-base').to(device)\n",
    "\n",
    "# Generate summaries for each class label\n",
    "for class_label in class_labels:\n",
    "    try:\n",
    "        # Filter test data for the current class label\n",
    "        test_data_class = test_df[test_df['label'] == class_label]['text'].tolist()\n",
    "        \n",
    "        # Tokenize and encode the text data for T5 input\n",
    "        input_ids_t5 = tokenizer_summarization.batch_encode_plus(test_data_class, \n",
    "                                                                 return_tensors='pt', \n",
    "                                                                 max_length=512, \n",
    "                                                                 truncation=True, \n",
    "                                                                 padding='longest').input_ids.to(device)\n",
    "        \n",
    "        # Generate summaries\n",
    "        with torch.no_grad():\n",
    "            output = model_summarization.generate(input_ids=input_ids_t5, \n",
    "                                                   max_length=150, \n",
    "                                                   num_beams=2, \n",
    "                                                   early_stopping=True)\n",
    "            \n",
    "        # Decode the generated summaries\n",
    "        summaries = [tokenizer_summarization.decode(summary, skip_special_tokens=True) for summary in output]\n",
    "\n",
    "        # Print the class label and its corresponding summaries\n",
    "        print(f\"Class Label: {class_label}\")\n",
    "        for summary in summaries:\n",
    "            print(summary)\n",
    "        print(\"-----------------------------------------------------\")\n",
    "    except ValueError:\n",
    "\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4725197,
     "sourceId": 8019295,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30702,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-23T22:19:26.818773",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}